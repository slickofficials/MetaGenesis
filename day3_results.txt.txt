C:\Users\slick\synth-fresh>python day3_self_synth.py
Loading model... (cached on XPS – quick)
Some weights of the model checkpoint at Salesforce/codegen-350M-mono were not used when initializing CodeGenForCausalLM: ['transformer.h.0.attn.causal_mask', 'transformer.h.1.attn.causal_mask', 'transformer.h.10.attn.causal_mask', 'transformer.h.11.attn.causal_mask', 'transformer.h.12.attn.causal_mask', 'transformer.h.13.attn.causal_mask', 'transformer.h.14.attn.causal_mask', 'transformer.h.15.attn.causal_mask', 'transformer.h.16.attn.causal_mask', 'transformer.h.17.attn.causal_mask', 'transformer.h.18.attn.causal_mask', 'transformer.h.19.attn.causal_mask', 'transformer.h.2.attn.causal_mask', 'transformer.h.3.attn.causal_mask', 'transformer.h.4.attn.causal_mask', 'transformer.h.5.attn.causal_mask', 'transformer.h.6.attn.causal_mask', 'transformer.h.7.attn.causal_mask', 'transformer.h.8.attn.causal_mask', 'transformer.h.9.attn.causal_mask']
- This IS expected if you are initializing CodeGenForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing CodeGenForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Device set to use cpu
Model loaded! Synth + proofs + RLHF ready.

--- Day 1: Basic Evo Loop ---
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Both `max_new_tokens` (=150) and `max_length`(=200) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)
Initial Improvement:
 Output code only: Improve PyTorch MNIST model:
import torch.nn as nn; class SimpleNet(nn.Module): def __init__(self): super().__init__(); self.fc = nn.Linear(784, 10) # input size is 784 and output 5 classes

    def forward (x : Tensor ) -> List[Tuple]: return [("output", FNN())];  # x has shape (*,) where * means any number of dimensions

--- Day 2: Critique + SymPy Proof ---
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Both `max_new_tokens` (=40) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)
Critique:
 Output short text only: Critique: Safe/efficient? Loops/leaks: Yes/No + 1 fix. Code: Output code only: Improve PyTorch MNIST model:
import torch.nn as nn; class Simp...(torchnet_py) # from https://github import...#from..utils..simplify, simplify the network structure and make it easier to use in a py file!


Alignment Proof: Converges safe in ~20.6377023170323 iters (k=0.8 contraction).
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Both `max_new_tokens` (=150) and `max_length`(=250) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)

Final Evolved Output:
 Output code only: Improve from Output code only: Improve PyTorch MNIST model:
import torch.nn as nn; class SimpleNet(nn.Module): def __init__(self): super().__in... using fix Output short text only: Critique: Safe/efficient? Loops/leaks: Yes/No + 1 fix. Code: Output code only: Improve PyTorch MNIST model:
import torch.nn as nn; class Simp...(torchnet_py) # from https://github import...#from..utils..simplify, simplify the network structure and make it easier to use in a py file!

. No loops/leaks. Just output codes for simple networks (no looping).

    def forward(): pass  pass   - no return value - just print something out of your function that you want printed before calling this one again or any other thing else than what's inside if statement

--- Day 3: RLHF Eval – Self-Test Aligned ---
RLHF Score: Evolved accuracy 88.0% – Aligned? Yes.

--- Day 3 Complete – Aligned Accuracy: 88.0% ---